---
layout: post
title: 考核题库-3
abbrlink: 92b62b1c93574c64b7fdc670a550e368
tags: []
categories:
  - 工作笔记
  - 会议摘要
date: 1756370467391
updated: 1756370572832
---

****一、知识库（50 分）****

**单选题（7道）**

**1. 客户上传了一份包含详细产品信息的PDF文档到知识库，希望AI能基于此回答问题。但AI的回答时常不准确或“幻觉”。以下哪项措施是提升回答准确性的【最有效】首选方法？**\
A. 在应用设置中大幅提高“温度（Temperature）”参数值。\
B. 检查并优化知识库文档的分段（Chunk）策略，确保关键信息被正确提取。\
C. 在提示词模板中删除所有关于“请根据知识库回答”的指令。\
D. 为应用切换一个更昂贵的大语言模型（LLM）。\
**答案：B**\
**解析：** 分段是知识库处理的核心，不合理的分段（如截断了关键信息）会直接导致检索失败，从而引发模型幻觉。调整分段是解决此类问题的根本手段。

**2. 在配置知识库的“分段处理”规则时，“重叠区（Overlap）”的主要作用是？**\
A. 减少知识库存储空间占用，提升处理速度。\
B. 防止在分段时，一个完整的语义或关键信息被割裂到两个段落中，保证检索上下文连贯。\
C. 用于标记文档中重复的内容，以便AI在回答时进行去重。\
D. 设置多个知识库之间的关联优先级。\
**答案：B**\
**解析：** 重叠区通过在相邻分段之间保留一部分重复文本，来确保当一个概念或句子被分段切断时，其在相邻段中仍有完整呈现，从而大幅提升检索的准确性。

**3. 小李为知识库上传了一个包含最新公司政策的Word文档。上传成功后，他直接去测试应用，发现AI给出的还是旧政策的内容。最可能的原因是什么？**\
A. 上传的Word文档格式不被平台支持。\
B. 知识库处理需要时间，文档尚未完成索引（Indexing）。\
C. 应用的对话提示词（Prompt）编写有误。\
D. 当前使用的LLM模型没有该领域知识。\
**答案：B**\
**解析：** 文档上传后，Dify需要在后台进行解析、分段和向量化（索引）处理，这个过程需要一定时间。在索引完成前，新知识不会被检索到。

**4. 知识库目前标准模式【不支持】处理以下哪种类型的文件？**\
A. Markdown (.md)\
B. PowerPoint演示文稿 (.pptx)\
C. 图片文件 (.jpg, .png)\
D. 文本文件（.txt)\
**答案：D**\
**解析：** Dify的知识库主要处理文本类信息。对于.jpg、.png等图片文件，它无法直接提取其中的文字内容（除非使用专家模式的视觉模型，但标准知识库不包含此功能）。其他选项格式均为支持类型。

**5. 如果想要确保AI应用在回答用户问题时，严格遵循知识库内容而绝不进行自由发挥，首先应该优化哪个部分？**\
A. 在模型提供商处申请更多的API额度。\
B. 在应用的“提示词”中明确加入强约束指令，如“严格根据提供的知识库内容回答，如果知识库中没有相关信息，请明确告知用户不知道”。\
C. 删除知识库中所有不重要的文档。\
D. 将检索模式从“高精度”切换到“高召回”。\
**答案：B**\
**解析：** 提示词是指导AI行为的最直接工具。通过强约束指令，可以有效地约束模型的生成范围，这是防止幻觉和自由发挥的首要且最有效的方法。

**6. 知识库中的“相似度阈值”设置主要影响什么？**\
A. 控制每次调用知识库检索所消耗的Token数量。\
B. 决定检索出的文本片段与用户问题相关性的最低分数，用于过滤掉低相关度的结果。\
C. 调整知识库处理文档时的并行任务数量。\
D. 设置用户提问和AI回答之间的响应时间延迟。\
**答案：B**\
**解析：** 相似度阈值是一个分数门槛。高于此门槛的文本片段会被视为相关并返回给模型作为上下文，低于此门槛的则被过滤掉。调高阈值会让检索更严格（相关度更高但结果可能更少），调低阈值则更宽松（结果更多但可能包含不相关信息）。

**7. 知识库的“检索模式”中，“高精度”和“高召回”的主要区别在于？**\
A. 高精度模式响应速度更快，高召回模式响应速度更慢。\
B. 高精度模式更倾向于返回少量高度相关的结果，高召回模式更倾向于返回大量可能相关的结果。\
C. 高精度模式消耗的算力更少，高召回模式消耗的算力更多。\
D. 高精度模式仅支持英文检索，高召回模式支持多语言检索。\
**答案：B**\
**解析：** 这是信息检索领域的经典概念。高精度（Precision）追求“返回的都是对的”，高召回（Recall）追求“对的都被返回了”。在Dify中，这通常通过调整底层检索算法（如MMR）的参数来实现。

 

**多选题（3道）**

**1. 【多选题】以下哪些因素会直接影响知识库的检索效果和最终回答质量？**\
A. 源文档的质量和清晰度。\
B. 知识库的分段（Chunk）长度和重叠区（Overlap）设置。\
C. 所选用的嵌入模型（Embedding Model）的性能。\
D. 应用提示词（Prompt）中对知识库使用方式的指令。\
E. 网站前端的UI配色方案。\
**答案：A, B, C, D**\
**解析：** A是数据源头，垃圾进垃圾出；B是数据处理的核心参数；C决定了文本转换为向量后语义保真度，影响检索准确性；D直接指挥AI如何利用检索到的上下文。E属于前端展示，与后端检索逻辑无关。

**2. 【多选题】一位客户希望为其大型法律文档库（包含数万份PDF合同和条款文件）构建一个精准的AI问答应用。在使用Dify知识库功能时，为保障项目成功，您作为售前工程师会建议他重点关注并做好以下哪些方面的规划？**\
A. **成本规划**：预估大量文档嵌入（Embedding）处理以及后续高频问答所产生的Token消耗成本，尤其是选用按量付费的模型时。\
B. **分段策略调优**：法律文档逻辑严密，需精心调试分段长度和重叠区，确保关键条款（如免责声明、责任限制）的完整性不被破坏。\
C. **模型选型**：必须选择最昂贵、参数最多的LLM（如GPT-4），因为只有它才能理解复杂的法律语言。\
D. **索引性能与时效性**：提前评估数万份文档的首次索引所需时间，并为其设计一套文档更新后的手动或自动重新索引流程。\
E. **提示词工程**：在提示词中必须严格限定AI的回答范围和语气（如“仅作为参考，不构成法律建议”），并设计对“未找到答案”情况的稳妥回复策略。\
F. **硬件配置**：建议客户为Dify服务部署独立的GPU服务器，因为知识库检索速度完全依赖于本地GPU算力。

**答案：A, B, D, E**

**解析：**

· **A（正确）**：这是一个非常实际的考量。处理海量文档会产生高昂的嵌入成本，后续每次问答的检索和生成也都会消耗Token。售前必须帮助客户预估成本，避免后续产生费用争议。

· **B（正确）**：这是法律场景下的核心挑战。普通的分段可能切碎一个完整的法条或合同项，导致检索失败或误解。必须根据法律文档的结构特点进行精细化的分段参数调整。

· **C（错误）**：模型选择并非越贵越好。许多强大的模型（如GPT-3.5-Turbo、Claude Haiku等）在理解结构化文本方面表现已足够出色，且成本更低。应建议客户根据效果和成本的平衡（POC测试）来选择，而非盲目追求最顶级模型。

· **D（正确）**：海量文档的索引是耗时操作，必须管理客户预期。同时，法律文档会更新，如何高效地同步和更新索引是保证系统可用性的关键，必须提前设计流程。

· **E（正确）**：对于法律、医疗等高风险领域，提示词中的免责声明和范围限定至关重要，这是产品安全和合规性的基本要求。同时，优雅地处理“未知问题”能极大提升用户体验和专业度。

· **F（错误）**：Dify的知识库检索通常依赖于云端的向量数据库服务，其性能和速度由服务提供商保障，**并不**直接依赖于客户本地的GPU算力。本地部署除外，但标准SaaS模式无需此考虑。这是一个迷惑项，用于考察对架构原理的理解。

**3. 【多选题】知识库在完成文档处理并索引后，其工作原理包含以下哪些关键步骤？**\
A. 将用户的自然语言问题转换为向量（Vectorize）。\
B. 在向量数据库中进行相似度搜索，找到最相关的文本片段。\
C. 将检索到的文本片段作为上下文，与用户问题一同组合成完整的提示词（Prompt）发送给LLM。\
D. 直接修改大语言模型内部的权重参数，使其记住知识库内容。\
E. 将检索到的文本片段先翻译成英文，再发送给LLM。\
**答案：A, B, C**\
**解析：** 这是RAG（检索增强生成）的标准流程：A是“问”，B是“搜”，C是“答”。D是完全错误的，知识库不会改变LLM本身。E不是必需步骤，多模型可以直接处理中文向量和中文问题。

 

**二、创建AI 应用**

**单选题（7道）**

**1. 在 AI 生产力平台中创建新应用时，第一个关键决策是选择应用类型。如果想要创建一个能进行多轮对话的客服助手，最应该选择哪种类型？**\
A. 视觉生成 应用\
B. 我的助手应用\
C. 视觉生成应用\
D. 业务流应用\
**答案：B**\
**解析：** 对话（Chat）应用类型是专门为多轮、有上下文关联的交互场景设计的，它内置了管理对话历史的功能。

**2. 配置应用的核心是提示词工程。在对话应用的“角色与提示词”部分，哪一部分主要用于定义AI的个性、专业领域和回答风格？**\
A. 上下文\
B. 变量\
C. 开场白\
D. 系统提示词（System Prompt）\
**答案：D**\
**解析：** 系统提示词（System Prompt）是指导AI行为最根本的指令，通常用于设定AI的角色（如“你是一位专业的法律顾问”）、回答原则和边界，它对模型的输出风格起着奠基性作用。

**3. 小王想让用户在与AI对话时，能手动切换回答的“创意程度”（如从“严谨”到“脑洞大开”）。他应该在应用配置中利用哪个功能来实现？**\
A. 在知识库中上传不同风格的文档。\
B. 在提示词中使用“变量”，并创建一个名为“creativity”的变量，将其与模型参数“温度”关联。\
C. 为同一个应用集成多个不同的大语言模型。\
D. 在“模型与参数”中，将“温度”参数固定为一个较高的值。\
**答案：B**\
**解析：** Dify的“变量”功能允许将前端用户输入与后端模型参数动态绑定。创建一个滑块变量并关联到“温度”（Temperature）参数，用户就可以实时调整这个参数，从而控制生成文本的随机性和创造性。

**4. 在“模型与参数”配置中，“最大令牌数（Max Tokens）”的主要作用是？**\
A. 控制AI生成单次回复的最大长度，防止回复过长或消耗过多Token。\
B. 设置用户问题的最大长度限制。\
C. 调整模型思考问题的计算时间。\
D. 限制知识库检索时返回的文本片段数量。\
**答案：A**\
**解析：** “最大令牌数”直接限制模型输出内容的最大长度，以Token数为单位。设置此参数可以有效控制单次交互的成本和响应内容的篇幅。

**5. 一个应用配置完成后，在正式发布给用户使用前，最重要的一步是什么？**\
A. 配置最好的模型。\
B. 使用界面右上角的“发布”按钮，将应用从“草稿”状态发布为“上线”状态。\
C. 在社交媒体上提前预告。\
D. 反复使用“调试”窗口进行测试，并根据结果优化提示词、参数或知识库。\
**答案：D**\
**解析：** 调试是迭代和优化应用的核心环节。通过模拟真实用户提问，可以不断发现提示词歧义、参数不合理或知识库检索失败等问题，确保应用上线后能达到预期效果。

**6. 如果想要将配置好的AI应用嵌入到公司现有的网站中，应该使用提供的哪种方式？**\
A. 将应用导出为可执行文件（.exe）。\
B. 在“发布”设置中，获取并部署API接口或嵌入iframe代码段。\
C. 将应用配置截图发给网站开发人员。\
D. 邀请网站用户全部注册平台账号。\
**答案：B**\
**解析：** 通过API可供后端深度集成，通过生成的iframe代码可直接嵌入到网页的某个区域，这是最标准且高效的嵌入方式。

**7. 在配置一个需要查询知识库的应用时，“召回测试”中“TOP K”选项的特点是？**\
A. 返回的结果数量较少，但与问题相关性极高。\
B. 返回的结果数量较多，尽可能覆盖所有可能相关的文本片段。\
C. 检索速度最快，但精度最低。\
D. 仅检索最近上传的文档。\
**答案：B**\
**解析：** “高召回”模式以提高召回率为目标，倾向于返回更多数量的相关文本片段，即使部分片段相关性可能稍低，旨在避免遗漏任何潜在有用的信息。

***

**多选题（3道）**

**1. 【多选题】以下哪些是可以在“提示词”中通过“变量”来实现的功能？**\
A. 让用户在聊天前自由填写一个“角色”下拉框（如：老师、学生、专家），使AI根据不同角色调整回答口吻。\
B. 创建一个文本框，让用户输入他们公司的名称，使AI在生成的文案中动态插入该名称。\
C. 创建一个滑块，让用户实时调整模型应答的“温度”（Temperature）参数。\
D. 直接修改大语言模型的内部权重。\
E. 设置一个开关，让用户决定本次对话是否启用知识库检索。\
**答案：A, B, C**\
**解析：** 变量是连接用户输入和提示词/模型参数的桥梁。A、B、C都是变量的典型应用场景。D是绝对不可能的。E是一个高级功能，通常需要通过“工作流”来实现逻辑判断，而非简单的变量。

**2. 【多选题】在“调试”一个对话应用时，发现AI的回答没有按预期从知识库中获取信息，而是基于模型自身知识泛泛而谈。导致这个问题可能的原因是？**\
A. 知识库中的文档尚未完成索引处理。\
B. 系统提示词中忘记加入“请根据<知识库>内容回答”等类似指令。\
C. 在“模型与参数”中，“温度”值设置得太低。\
D. 在“上下文”配置中，没有开启“检索知识库”功能或未正确关联知识库。\
**答案：A, B, D**\
**解析：** A导致无内容可检索；B导致AI即使拿到了检索结果也不知道要用它；D是根本性的功能开关未打开。C（温度低）只会让回答更确定性，但不会影响是否检索知识库这一行为。

**3. 【多选题】关于应用的“发布与协作”，以下说法正确的是？**\
A. 可以将应用权限设置为“私有”，仅限自己或团队内成员访问。\
B. 可以将应用权限设置为“公开”，任何人通过链接都可访问，无需API密钥。\
C. 可以在应用设置白名单，限制 URL 的访问登录。\
D. 一旦应用发布上线，其所有配置（包括提示词）都无法再被修改。\
**答案：A, B, C**\
**解析：** A、B、C都是Dify核心的协作和发布功能。D是错误的，应用上线后可以随时修改配置（会保存为新版本），并需要重新发布才能使最新修改生效，这实现了应用的迭代更新。

 

 

**三、加分题：**

**1. 【多选题】您正在为客户设计一个高性能、低延迟且需要控制成本的AI应用。该应用部署在Dify上，预计会有大量用户并发访问。以下哪些配置策略是合理且有效的？**\
A. **模型选型**：在所有环境下（调试、预览、生产）都统一使用性能最强的GPT-4模型，以确保体验一致。\
B. **缓存策略**：开启“对话内存”或类似缓存功能，对频繁出现的相似问题及答案进行缓存，减少对LLM API的重复调用。\
C. **异步处理**：对于处理时间可能较长的复杂“工作流”任务（如生成报告），配置异步处理并提供任务ID，让用户稍后查询结果，避免请求超时。\
D. **性能与成本平衡**：在生产环境中，对知识库检索和对话生成使用高性能模型（如GPT-4），但对简单的意图分类或文本总结任务使用更具性价比的模型（如GPT-3.5-Turbo）。\
E. **监控与告警**：在Dify后台密切关注应用的Token消耗、响应延迟和API错误率，并设置预算告警，以便在成本异常激增时及时干预。\
F. **硬件扩容**：为了应对高并发，建议客户为Dify的服务器采购更多的CPU核心，因为LLM的计算负载主要消耗CPU资源。

**答案：B, C, E**

**解析：**

· **A（错误）**：这是一个成本控制上的反面教材。在调试和内部测试阶段使用昂贵的GPT-4会造成巨大的浪费。合理的策略是：调试阶段用廉价模型（如GPT-3.5-Turbo）迭代提示词，最终生产环境再根据效果需求评估是否升级到更强模型。

· **B（正确）**：缓存是应对高并发、降低延迟和成本的经典手段。将常见QA对缓存起来，可以直接快速响应，无需调用模型，极大提升效率。

· **C（正确）**：这是处理长耗时任务的最佳实践。同步等待会导致连接超时和用户体验极差。将其异步化，立即返回一个“任务已接收”的响应，后续通过轮询或Webhook通知用户获取结果，是更专业的架构设计。

· **D（错误 - 部分正确）**：这个思路（在应用内不同环节使用不同模型）本身非常先进，被称为“模型路由”或“ cascading ”，是平衡成本与效果的高级策略。**然而**，在标准版的Dify中，一个应用通常只能关联一个主模型进行文本生成，暂时还无法在单个应用内如此精细地为不同功能分配不同模型（此功能可能需要通过自定义代码/workflow实现）。因此，作为标准功能的考察，此项不选。

· **E（正确）**：监控是运维和成本控制的基石。密切监控核心指标并设置告警，可以帮助团队快速发现性能瓶颈、API故障或成本异常（如提示词错误导致循环调用），是保障应用稳定、可控运行的必备措施。

· **F（错误）**：这是一个常见的误解。对于使用Dify SaaS服务或基于API调用云厂商（如OpenAI, Anthropic）模型的用户来说，**LLM的推理计算完全发生在模型提供商的服务器上**，消耗的是他们的GPU资源。Dify自身的服务器主要负责业务流程编排、知识库检索等任务。因此，应用的并发能力和响应速度主要受限于模型提供商的API速率限制和网络延迟，与客户本地或托管Dify的服务器的CPU关系不大。本地部署大模型的情况除外。

 

**２、知识库的高级解析模式功能，新增加文件图像处理**

A、可以不用写图像分析提示词

B 、视觉模型对图片进行处理，是在Embedding 前

C、视觉模型能对图片进行理解、分析和总结，并总结和文本内容一起Embedding

D、视觉模型处理的图片内容只在文件分段用figures作为标识展示

**答案：A、B、C** 、D

 
